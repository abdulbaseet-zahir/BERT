# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
**[The paper](https://arxiv.org/abs/1810.04805)**
